{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nov05/Google-Colaboratory/blob/master/20241129_finetune_bert_solution_2_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "079808d1",
      "metadata": {
        "id": "079808d1"
      },
      "source": [
        "* changed by nov05 on 2024-11-28  \n",
        "* Udacity AWS MLE Nanodegree (ND189)  \n",
        "  Course 4, 3.15 Excercise: Fine-Tuning BERT    \n",
        "* local env `conda activate cuda_py310` with cuda enabled  \n",
        "* [CoLA dataset on KaggleHub](https://www.kaggle.com/datasets/krazy47/cola-the-corpus-of-linguistic-acceptability)  \n",
        "  The Corpus of Linguistic Acceptability   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "kaD2qZyV2Abt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaD2qZyV2Abt",
        "outputId": "f1bdad95-3022-4d9a-dfaf-65abd7f51d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-29 13:12:54--  https://raw.githubusercontent.com/nov05/udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter/refs/heads/main/cd0387_common_model_arch_types_fine_tuning/cola_public/raw/in_domain_train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 428578 (419K) [text/plain]\n",
            "Saving to: ‘cola_public/raw/in_domain_train.tsv’\n",
            "\n",
            "\r          cola_publ   0%[                    ]       0  --.-KB/s               \rcola_public/raw/in_ 100%[===================>] 418.53K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-11-29 13:12:54 (12.1 MB/s) - ‘cola_public/raw/in_domain_train.tsv’ saved [428578/428578]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p cola_public\n",
        "!mkdir -p cola_public/raw\n",
        "!wget https://raw.githubusercontent.com/nov05/udacity-CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter/refs/heads/main/cd0387_common_model_arch_types_fine_tuning/cola_public/raw/in_domain_train.tsv -O cola_public/raw/in_domain_train.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece7b442",
      "metadata": {
        "id": "ece7b442"
      },
      "source": [
        "## Solution: Fine-tune BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "20247d44",
      "metadata": {
        "id": "20247d44"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# import sys\n",
        "# import json\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "# import torch.distributed as dist\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer # type: ignore\n",
        "from transformers import get_linear_schedule_with_warmup # type: ignore\n",
        "from sklearn.model_selection import train_test_split # type: ignore\n",
        "from sklearn.utils.class_weight import compute_class_weight # type: ignore\n",
        "\n",
        "## log training process with W&B if uncommented\n",
        "# os.environ['WANDB_MODE'] = 'disabled'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "23828261",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23828261",
        "outputId": "d59c14c7-f547-401e-94ae-923f4be5f19c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "👉 Running on device type: cuda:0\n"
          ]
        }
      ],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.wandb = True\n",
        "        self.device = torch.device('cpu')\n",
        "        self.max_len = 64 ## this is the max length of the sentence\n",
        "        self.epochs = 15\n",
        "        self.batch_size = 64\n",
        "        self.opt_lr = 2e-5\n",
        "\n",
        "config = Config()\n",
        "config.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"👉 Running on device type: {config.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9a15cc9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "9a15cc9a",
        "outputId": "c062f548-7c10-4f56-a01f-eafe0f6dfc24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8551, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      label                                           sentence\n",
              "1507      1  Do you believe that somebody was looking for s...\n",
              "4444      0  Americans have paying income tax ever since 1913.\n",
              "6507      1    Almost every cat likes mice, but Felix doesn't."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e009b10-7e9c-43cf-a8b3-5ab7ac8be325\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1507</th>\n",
              "      <td>1</td>\n",
              "      <td>Do you believe that somebody was looking for s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4444</th>\n",
              "      <td>0</td>\n",
              "      <td>Americans have paying income tax ever since 1913.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6507</th>\n",
              "      <td>1</td>\n",
              "      <td>Almost every cat likes mice, but Felix doesn't.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e009b10-7e9c-43cf-a8b3-5ab7ac8be325')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0e009b10-7e9c-43cf-a8b3-5ab7ac8be325 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0e009b10-7e9c-43cf-a8b3-5ab7ac8be325');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c17d819f-697c-4085-938b-175dc724fcd7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c17d819f-697c-4085-938b-175dc724fcd7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c17d819f-697c-4085-938b-175dc724fcd7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Do you believe that somebody was looking for something?\",\n          \"Americans have paying income tax ever since 1913.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df = pd.read_csv(\n",
        "   r\"/content/cola_public/raw/in_domain_train.tsv\",\n",
        "   sep=\"\\t\",\n",
        "   header=None,\n",
        "   usecols=[1, 3],\n",
        "   names=[\"label\", \"sentence\"],\n",
        ")\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "print(df.shape)\n",
        "df.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1c7d8d8c",
      "metadata": {
        "id": "1c7d8d8c"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "!mkdir -p data/cola\n",
        "train_df, test_df = train_test_split(df, stratify=labels)\n",
        "train_df.to_csv(r\"data/cola/train.csv\", index=False)\n",
        "test_df.to_csv(r\"data/cola/test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7e22c3cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e22c3cf",
        "outputId": "d7671be9-254d-475f-d3ed-5d3cd6957aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading BERT tokenizer...\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat==labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "def get_train_data_loader(batch_size):\n",
        "    dataset = pd.read_csv(os.path.join(\"data\", \"cola\", \"train.csv\"))\n",
        "    sentences = dataset.sentence.values\n",
        "    labels = dataset.label.values\n",
        "\n",
        "    input_ids = []\n",
        "    for sent in sentences:\n",
        "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
        "        input_ids.append(encoded_sent)\n",
        "\n",
        "    # pad shorter sentences\n",
        "    input_ids_padded = []\n",
        "    for id in input_ids:\n",
        "        while len(id) < config.max_len:\n",
        "            id.append(0)\n",
        "        input_ids_padded.append(id)\n",
        "    input_ids = input_ids_padded\n",
        "\n",
        "    # mask; 0: added, 1: otherwise\n",
        "    attention_masks = []\n",
        "    # For each sentence...\n",
        "    for sent in input_ids:\n",
        "        att_mask = [int(token_id > 0) for token_id in sent]\n",
        "        attention_masks.append(att_mask)\n",
        "\n",
        "    # convert to PyTorch data types.\n",
        "    train_inputs = torch.tensor(input_ids)\n",
        "    train_labels = torch.tensor(labels)\n",
        "    train_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader\n",
        "\n",
        "\n",
        "def get_test_data_loader(test_batch_size):\n",
        "    dataset = pd.read_csv(os.path.join(\"data\", \"cola\", \"test.csv\"))\n",
        "    sentences = dataset.sentence.values\n",
        "    labels = dataset.label.values\n",
        "\n",
        "    input_ids = []\n",
        "    for sent in sentences:\n",
        "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
        "        input_ids.append(encoded_sent)\n",
        "\n",
        "    # pad shorter sentences\n",
        "    input_ids_padded = []\n",
        "    for id in input_ids:\n",
        "        while len(id)<config.max_len:\n",
        "            id.append(0)\n",
        "        input_ids_padded.append(id)\n",
        "    input_ids = input_ids_padded\n",
        "\n",
        "    # mask; 0: added, 1: otherwise\n",
        "    attention_masks = []\n",
        "    # For each sentence...\n",
        "    for sent in input_ids:\n",
        "        att_mask = [int(token_id>0) for token_id in sent]\n",
        "        attention_masks.append(att_mask)\n",
        "\n",
        "    # convert to PyTorch data types.\n",
        "    train_inputs = torch.tensor(input_ids)\n",
        "    train_labels = torch.tensor(labels)\n",
        "    train_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\n",
        "\n",
        "    return train_dataloader\n",
        "\n",
        "\n",
        "def train():\n",
        "    train_loader = get_train_data_loader(config.batch_size)\n",
        "    test_loader = get_test_data_loader(config.batch_size)\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
        "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
        "        output_attentions=False,  # Whether the model returns attentions weights\n",
        "        output_hidden_states=False,  # Whether the model returns all hidden-states\n",
        "    )\n",
        "    model = model.to(config.device)\n",
        "    optimizer = AdamW(model.parameters(), lr=config.opt_lr)\n",
        "    total_steps = 0\n",
        "\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        print(f\"👉 Train Epoch {epoch}:\")\n",
        "        loss_epoch = 0\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            total_steps += 1\n",
        "            b_input_ids = batch[0].to(config.device)\n",
        "            b_input_mask = batch[1].to(config.device)\n",
        "            b_labels = batch[2].to(config.device)\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                b_input_ids,  ## Shape: (batch_size, sequence_length)\n",
        "                token_type_ids=None, ## Shape: (batch_size, sequence_length)\n",
        "                attention_mask=b_input_mask, ## Shape: (batch_size, sequence_length)\n",
        "                labels=b_labels) ## Shape: (batch_size,)\n",
        "\n",
        "            loss = outputs.loss  ## same with outputs[0]\n",
        "            wandb.log({\"train_loss\": loss.item()}, step=total_steps)\n",
        "            loss_epoch += loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "            optimizer.step()\n",
        "            if step%10==0:\n",
        "                print(\n",
        "                    f\"Step {total_steps}: \"\n",
        "                    f\"[{step*len(batch[0])}/{len(train_loader.sampler)} \"\n",
        "                    f\"({(100.0*step/len(train_loader)):.0f}%)] \"\n",
        "                    f\"Loss: {loss.item():.6f}\"\n",
        "                )\n",
        "        wandb.log({\"train_loss_epoch\": loss_epoch/config.batch_size}, step=total_steps)\n",
        "        eval_accuracy = test(model, test_loader)\n",
        "        wandb.log({f\"eval_accuracy_epoch (%)\": eval_accuracy*100}, step=total_steps)\n",
        "    return model\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    eval_accuracy_steps = 0\n",
        "    total_steps = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            total_steps += 1\n",
        "            b_input_ids = batch[0].to(config.device)\n",
        "            b_input_mask = batch[1].to(config.device)\n",
        "            b_labels = batch[2].to(config.device)\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "            logits = outputs.logits.detach().cpu().numpy()  ## differs from the original\n",
        "            label_ids = b_labels.to(\"cpu\").numpy()\n",
        "            eval_accuracy_steps += flat_accuracy(logits, label_ids)\n",
        "    eval_accuracy = eval_accuracy_steps / total_steps\n",
        "    print(\"\\n🟢 Test Accuracy (%): \", eval_accuracy*100)\n",
        "    return eval_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"udacity-awsmle-bert-cola\",\n",
        "    config=config\n",
        ")\n",
        "train()\n",
        "## 15 epochs, 16:49 mins"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "38d9830025d248e18b6049f8ba921001",
            "cf8e4587853f47d09a653a3d36415035",
            "04fb7cc1fe134b789f9168bd4b0771ce",
            "9620104e50614f929210abec730513eb",
            "2474b2c7b1714126b182b1c1eddbda79",
            "ff21894d08964a5abd5ef266ae23e723",
            "1691f052937b4473a8e79f64554cdb19",
            "e56dc9131998422bab4415c3eb74cb38"
          ]
        },
        "id": "sbM4osY5CGem",
        "outputId": "26f97d4a-29b4-47f9-d590-8378dc02f2c7"
      },
      "id": "sbM4osY5CGem",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:k52c4yk6) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.270 MB of 0.270 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38d9830025d248e18b6049f8ba921001"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>▆█▅▄▇▆▄▇▇▇▄▆█▅▆▅▅▅▆▃▂▄▅▆▂▄▅▃▆▆▂▂▄▅▃▄▂▁▁▄</td></tr><tr><td>train_loss_epoch</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.42</td></tr><tr><td>train_loss_epoch</td><td>0.82879</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glowing-mountain-19</strong> at: <a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/k52c4yk6' target=\"_blank\">https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/k52c4yk6</a><br/> View project at: <a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola' target=\"_blank\">https://wandb.ai/nov05/udacity-awsmle-bert-cola</a><br/>Synced 4 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241129_131034-k52c4yk6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:k52c4yk6). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241129_131300-3blkneau</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/3blkneau' target=\"_blank\">solar-thunder-20</a></strong> to <a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola' target=\"_blank\">https://wandb.ai/nov05/udacity-awsmle-bert-cola</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/3blkneau' target=\"_blank\">https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/3blkneau</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "👉 Train Epoch 0:\n",
            "Step 1: [0/6413 (0%)] Loss: 1.015233\n",
            "Step 11: [640/6413 (10%)] Loss: 0.596859\n",
            "Step 21: [1280/6413 (20%)] Loss: 0.577084\n",
            "Step 31: [1920/6413 (30%)] Loss: 0.561362\n",
            "Step 41: [2560/6413 (40%)] Loss: 0.511488\n",
            "Step 51: [3200/6413 (50%)] Loss: 0.428192\n",
            "Step 61: [3840/6413 (59%)] Loss: 0.448093\n",
            "Step 71: [4480/6413 (69%)] Loss: 0.438618\n",
            "Step 81: [5120/6413 (79%)] Loss: 0.460993\n",
            "Step 91: [5760/6413 (89%)] Loss: 0.498080\n",
            "Step 101: [1300/6413 (99%)] Loss: 0.379479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 1/15 [01:05<15:23, 65.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  79.05472285067874\n",
            "👉 Train Epoch 1:\n",
            "Step 102: [0/6413 (0%)] Loss: 0.340742\n",
            "Step 112: [640/6413 (10%)] Loss: 0.342892\n",
            "Step 122: [1280/6413 (20%)] Loss: 0.435111\n",
            "Step 132: [1920/6413 (30%)] Loss: 0.422198\n",
            "Step 142: [2560/6413 (40%)] Loss: 0.389161\n",
            "Step 152: [3200/6413 (50%)] Loss: 0.468284\n",
            "Step 162: [3840/6413 (59%)] Loss: 0.437216\n",
            "Step 172: [4480/6413 (69%)] Loss: 0.310340\n",
            "Step 182: [5120/6413 (79%)] Loss: 0.437729\n",
            "Step 192: [5760/6413 (89%)] Loss: 0.377489\n",
            "Step 202: [1300/6413 (99%)] Loss: 0.269135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [02:11<14:12, 65.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  80.88588800904978\n",
            "👉 Train Epoch 2:\n",
            "Step 203: [0/6413 (0%)] Loss: 0.176743\n",
            "Step 213: [640/6413 (10%)] Loss: 0.253901\n",
            "Step 223: [1280/6413 (20%)] Loss: 0.253091\n",
            "Step 233: [1920/6413 (30%)] Loss: 0.183049\n",
            "Step 243: [2560/6413 (40%)] Loss: 0.258893\n",
            "Step 253: [3200/6413 (50%)] Loss: 0.218185\n",
            "Step 263: [3840/6413 (59%)] Loss: 0.239339\n",
            "Step 273: [4480/6413 (69%)] Loss: 0.259622\n",
            "Step 283: [5120/6413 (79%)] Loss: 0.423789\n",
            "Step 293: [5760/6413 (89%)] Loss: 0.249109\n",
            "Step 303: [1300/6413 (99%)] Loss: 0.104558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [03:16<13:06, 65.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  81.12273755656109\n",
            "👉 Train Epoch 3:\n",
            "Step 304: [0/6413 (0%)] Loss: 0.116179\n",
            "Step 314: [640/6413 (10%)] Loss: 0.083321\n",
            "Step 324: [1280/6413 (20%)] Loss: 0.058799\n",
            "Step 334: [1920/6413 (30%)] Loss: 0.184692\n",
            "Step 344: [2560/6413 (40%)] Loss: 0.115744\n",
            "Step 354: [3200/6413 (50%)] Loss: 0.151370\n",
            "Step 364: [3840/6413 (59%)] Loss: 0.154031\n",
            "Step 374: [4480/6413 (69%)] Loss: 0.283659\n",
            "Step 384: [5120/6413 (79%)] Loss: 0.072807\n",
            "Step 394: [5760/6413 (89%)] Loss: 0.177430\n",
            "Step 404: [1300/6413 (99%)] Loss: 0.018000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [04:22<12:00, 65.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  81.00608031674209\n",
            "👉 Train Epoch 4:\n",
            "Step 405: [0/6413 (0%)] Loss: 0.184099\n",
            "Step 415: [640/6413 (10%)] Loss: 0.039550\n",
            "Step 425: [1280/6413 (20%)] Loss: 0.090187\n",
            "Step 435: [1920/6413 (30%)] Loss: 0.214358\n",
            "Step 445: [2560/6413 (40%)] Loss: 0.010570\n",
            "Step 455: [3200/6413 (50%)] Loss: 0.170485\n",
            "Step 465: [3840/6413 (59%)] Loss: 0.039865\n",
            "Step 475: [4480/6413 (69%)] Loss: 0.141023\n",
            "Step 485: [5120/6413 (79%)] Loss: 0.198768\n",
            "Step 495: [5760/6413 (89%)] Loss: 0.131117\n",
            "Step 505: [1300/6413 (99%)] Loss: 0.022869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [05:27<10:55, 65.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  80.88942307692308\n",
            "👉 Train Epoch 5:\n",
            "Step 506: [0/6413 (0%)] Loss: 0.012800\n",
            "Step 516: [640/6413 (10%)] Loss: 0.013958\n",
            "Step 526: [1280/6413 (20%)] Loss: 0.105263\n",
            "Step 536: [1920/6413 (30%)] Loss: 0.021364\n",
            "Step 546: [2560/6413 (40%)] Loss: 0.222037\n",
            "Step 556: [3200/6413 (50%)] Loss: 0.055045\n",
            "Step 566: [3840/6413 (59%)] Loss: 0.090289\n",
            "Step 576: [4480/6413 (69%)] Loss: 0.066834\n",
            "Step 586: [5120/6413 (79%)] Loss: 0.068016\n",
            "Step 596: [5760/6413 (89%)] Loss: 0.108372\n",
            "Step 606: [1300/6413 (99%)] Loss: 0.006313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [06:33<09:49, 65.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  80.66317873303169\n",
            "👉 Train Epoch 6:\n",
            "Step 607: [0/6413 (0%)] Loss: 0.022373\n",
            "Step 617: [640/6413 (10%)] Loss: 0.086619\n",
            "Step 627: [1280/6413 (20%)] Loss: 0.186906\n",
            "Step 637: [1920/6413 (30%)] Loss: 0.006918\n",
            "Step 647: [2560/6413 (40%)] Loss: 0.058736\n",
            "Step 657: [3200/6413 (50%)] Loss: 0.027749\n",
            "Step 667: [3840/6413 (59%)] Loss: 0.008781\n",
            "Step 677: [4480/6413 (69%)] Loss: 0.117816\n",
            "Step 687: [5120/6413 (79%)] Loss: 0.196952\n",
            "Step 697: [5760/6413 (89%)] Loss: 0.078906\n",
            "Step 707: [1300/6413 (99%)] Loss: 0.021127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [07:38<08:43, 65.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  80.43339932126698\n",
            "👉 Train Epoch 7:\n",
            "Step 708: [0/6413 (0%)] Loss: 0.048756\n",
            "Step 718: [640/6413 (10%)] Loss: 0.066421\n",
            "Step 728: [1280/6413 (20%)] Loss: 0.063138\n",
            "Step 738: [1920/6413 (30%)] Loss: 0.096319\n",
            "Step 748: [2560/6413 (40%)] Loss: 0.089698\n",
            "Step 758: [3200/6413 (50%)] Loss: 0.159782\n",
            "Step 768: [3840/6413 (59%)] Loss: 0.042548\n",
            "Step 778: [4480/6413 (69%)] Loss: 0.002882\n",
            "Step 788: [5120/6413 (79%)] Loss: 0.175485\n",
            "Step 798: [5760/6413 (89%)] Loss: 0.050303\n",
            "Step 808: [1300/6413 (99%)] Loss: 0.004666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [08:44<07:38, 65.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  81.11920248868778\n",
            "👉 Train Epoch 8:\n",
            "Step 809: [0/6413 (0%)] Loss: 0.050057\n",
            "Step 819: [640/6413 (10%)] Loss: 0.029290\n",
            "Step 829: [1280/6413 (20%)] Loss: 0.077043\n",
            "Step 839: [1920/6413 (30%)] Loss: 0.016503\n",
            "Step 849: [2560/6413 (40%)] Loss: 0.180613\n",
            "Step 859: [3200/6413 (50%)] Loss: 0.009582\n",
            "Step 869: [3840/6413 (59%)] Loss: 0.035849\n",
            "Step 879: [4480/6413 (69%)] Loss: 0.131783\n",
            "Step 889: [5120/6413 (79%)] Loss: 0.104388\n",
            "Step 899: [5760/6413 (89%)] Loss: 0.069340\n",
            "Step 909: [1300/6413 (99%)] Loss: 0.002380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 9/15 [09:49<06:32, 65.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  80.84346719457014\n",
            "👉 Train Epoch 9:\n",
            "Step 910: [0/6413 (0%)] Loss: 0.033285\n",
            "Step 920: [640/6413 (10%)] Loss: 0.007830\n",
            "Step 930: [1280/6413 (20%)] Loss: 0.141659\n",
            "Step 940: [1920/6413 (30%)] Loss: 0.031147\n",
            "Step 950: [2560/6413 (40%)] Loss: 0.001346\n",
            "Step 960: [3200/6413 (50%)] Loss: 0.001310\n",
            "Step 970: [3840/6413 (59%)] Loss: 0.089600\n",
            "Step 980: [4480/6413 (69%)] Loss: 0.229051\n",
            "Step 990: [5120/6413 (79%)] Loss: 0.083638\n",
            "Step 1000: [5760/6413 (89%)] Loss: 0.005009\n",
            "Step 1010: [1300/6413 (99%)] Loss: 0.001334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 10/15 [10:54<05:27, 65.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  81.05203619909503\n",
            "👉 Train Epoch 10:\n",
            "Step 1011: [0/6413 (0%)] Loss: 0.042690\n",
            "Step 1021: [640/6413 (10%)] Loss: 0.087750\n",
            "Step 1031: [1280/6413 (20%)] Loss: 0.052809\n",
            "Step 1041: [1920/6413 (30%)] Loss: 0.166128\n",
            "Step 1051: [2560/6413 (40%)] Loss: 0.089718\n",
            "Step 1061: [3200/6413 (50%)] Loss: 0.090898\n",
            "Step 1071: [3840/6413 (59%)] Loss: 0.021945\n",
            "Step 1081: [4480/6413 (69%)] Loss: 0.010999\n",
            "Step 1091: [5120/6413 (79%)] Loss: 0.085010\n",
            "Step 1101: [5760/6413 (89%)] Loss: 0.119327\n",
            "Step 1111: [1300/6413 (99%)] Loss: 0.000901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 11/15 [12:00<04:21, 65.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  81.83328619909503\n",
            "👉 Train Epoch 11:\n",
            "Step 1112: [0/6413 (0%)] Loss: 0.073044\n",
            "Step 1122: [640/6413 (10%)] Loss: 0.024650\n",
            "Step 1132: [1280/6413 (20%)] Loss: 0.057917\n",
            "Step 1142: [1920/6413 (30%)] Loss: 0.002120\n",
            "Step 1152: [2560/6413 (40%)] Loss: 0.003340\n",
            "Step 1162: [3200/6413 (50%)] Loss: 0.001207\n",
            "Step 1172: [3840/6413 (59%)] Loss: 0.029072\n",
            "Step 1182: [4480/6413 (69%)] Loss: 0.047907\n",
            "Step 1192: [5120/6413 (79%)] Loss: 0.009186\n",
            "Step 1202: [5760/6413 (89%)] Loss: 0.086555\n",
            "Step 1212: [1300/6413 (99%)] Loss: 0.049345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [13:05<03:16, 65.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  80.46167986425338\n",
            "👉 Train Epoch 12:\n",
            "Step 1213: [0/6413 (0%)] Loss: 0.048198\n",
            "Step 1223: [640/6413 (10%)] Loss: 0.009345\n",
            "Step 1233: [1280/6413 (20%)] Loss: 0.059091\n",
            "Step 1243: [1920/6413 (30%)] Loss: 0.001085\n",
            "Step 1253: [2560/6413 (40%)] Loss: 0.001310\n",
            "Step 1263: [3200/6413 (50%)] Loss: 0.002025\n",
            "Step 1273: [3840/6413 (59%)] Loss: 0.007220\n",
            "Step 1283: [4480/6413 (69%)] Loss: 0.011646\n",
            "Step 1293: [5120/6413 (79%)] Loss: 0.000497\n",
            "Step 1303: [5760/6413 (89%)] Loss: 0.053185\n",
            "Step 1313: [1300/6413 (99%)] Loss: 0.079911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [14:11<02:10, 65.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  81.64946266968326\n",
            "👉 Train Epoch 13:\n",
            "Step 1314: [0/6413 (0%)] Loss: 0.000352\n",
            "Step 1324: [640/6413 (10%)] Loss: 0.001528\n",
            "Step 1334: [1280/6413 (20%)] Loss: 0.017987\n",
            "Step 1344: [1920/6413 (30%)] Loss: 0.002990\n",
            "Step 1354: [2560/6413 (40%)] Loss: 0.011451\n",
            "Step 1364: [3200/6413 (50%)] Loss: 0.000464\n",
            "Step 1374: [3840/6413 (59%)] Loss: 0.158036\n",
            "Step 1384: [4480/6413 (69%)] Loss: 0.016970\n",
            "Step 1394: [5120/6413 (79%)] Loss: 0.004681\n",
            "Step 1404: [5760/6413 (89%)] Loss: 0.002707\n",
            "Step 1414: [1300/6413 (99%)] Loss: 0.001491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [15:16<01:05, 65.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  80.89649321266968\n",
            "👉 Train Epoch 14:\n",
            "Step 1415: [0/6413 (0%)] Loss: 0.002144\n",
            "Step 1425: [640/6413 (10%)] Loss: 0.006059\n",
            "Step 1435: [1280/6413 (20%)] Loss: 0.001450\n",
            "Step 1445: [1920/6413 (30%)] Loss: 0.068424\n",
            "Step 1455: [2560/6413 (40%)] Loss: 0.000472\n",
            "Step 1465: [3200/6413 (50%)] Loss: 0.000764\n",
            "Step 1475: [3840/6413 (59%)] Loss: 0.030084\n",
            "Step 1485: [4480/6413 (69%)] Loss: 0.098110\n",
            "Step 1495: [5120/6413 (79%)] Loss: 0.000306\n",
            "Step 1505: [5760/6413 (89%)] Loss: 0.001544\n",
            "Step 1515: [1300/6413 (99%)] Loss: 0.000132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [16:21<00:00, 65.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Test Accuracy (%):  81.44089366515837\n",
            "CPU times: user 16min 18s, sys: 1.89 s, total: 16min 20s\n",
            "Wall time: 16min 32s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "pN9CK3SpFrp2",
        "outputId": "b2df6e95-b078-4243-ff91-85e3f9b5647d"
      },
      "id": "pN9CK3SpFrp2",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy_epoch (%)</td><td>▁▆▆▆▆▅▄▆▆▆█▅█▆▇</td></tr><tr><td>train_loss</td><td>██▅▇▃▄▃▃▃▃▁▂▂▁▁▂▁▂▁▁▂▁▁▂▁▁▁▂▂▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy_epoch (%)</td><td>81.44089</td></tr><tr><td>train_loss</td><td>0.00013</td></tr><tr><td>train_loss_epoch</td><td>0.02814</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">solar-thunder-20</strong> at: <a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/3blkneau' target=\"_blank\">https://wandb.ai/nov05/udacity-awsmle-bert-cola/runs/3blkneau</a><br/> View project at: <a href='https://wandb.ai/nov05/udacity-awsmle-bert-cola' target=\"_blank\">https://wandb.ai/nov05/udacity-awsmle-bert-cola</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241129_131300-3blkneau/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07bf3539",
      "metadata": {
        "id": "07bf3539"
      },
      "source": [
        "```python\n",
        "BertForSequenceClassification(\n",
        "  (bert): BertModel(\n",
        "    (embeddings): BertEmbeddings(\n",
        "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
        "      (position_embeddings): Embedding(512, 768)\n",
        "      (token_type_embeddings): Embedding(2, 768)\n",
        "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "      (dropout): Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "    (encoder): BertEncoder(\n",
        "      (layer): ModuleList(\n",
        "        (0-11): 12 x BertLayer(\n",
        "          (attention): BertAttention(\n",
        "            (self): BertSdpaSelfAttention(\n",
        "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (dropout): Dropout(p=0.1, inplace=False)\n",
        "            )\n",
        "            (output): BertSelfOutput(\n",
        "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "              (dropout): Dropout(p=0.1, inplace=False)\n",
        "            )\n",
        "          )\n",
        "          (intermediate): BertIntermediate(\n",
        "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "            (intermediate_act_fn): GELUActivation()\n",
        "          )\n",
        "          (output): BertOutput(\n",
        "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "            (dropout): Dropout(p=0.1, inplace=False)\n",
        "          )\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "    (pooler): BertPooler(\n",
        "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "      (activation): Tanh()\n",
        "    )\n",
        "  )\n",
        "  (dropout): Dropout(p=0.1, inplace=False)\n",
        "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wmMGvdGFFy-a",
      "metadata": {
        "id": "wmMGvdGFFy-a"
      },
      "source": [
        "* [Unfreezing all layers of BERT giving good results than freezing and adding custom Forward layer for Fine-Tuning](https://www.reddit.com/r/MLQuestions/comments/1d07qlz/unfreezing_all_layers_of_bert_giving_good_results/)   \n",
        "* [I just can't fine tune BERT over 40% accuracy for text-classification task](https://www.reddit.com/r/MachineLearning/comments/1bx5r8r/d_i_just_cant_fine_tune_bert_over_40_accuracy_for/)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "rC-aghUPA6WY",
      "metadata": {
        "id": "rC-aghUPA6WY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38d9830025d248e18b6049f8ba921001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf8e4587853f47d09a653a3d36415035",
              "IPY_MODEL_04fb7cc1fe134b789f9168bd4b0771ce"
            ],
            "layout": "IPY_MODEL_9620104e50614f929210abec730513eb"
          }
        },
        "cf8e4587853f47d09a653a3d36415035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2474b2c7b1714126b182b1c1eddbda79",
            "placeholder": "​",
            "style": "IPY_MODEL_ff21894d08964a5abd5ef266ae23e723",
            "value": "0.318 MB of 0.318 MB uploaded\r"
          }
        },
        "04fb7cc1fe134b789f9168bd4b0771ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1691f052937b4473a8e79f64554cdb19",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e56dc9131998422bab4415c3eb74cb38",
            "value": 1
          }
        },
        "9620104e50614f929210abec730513eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2474b2c7b1714126b182b1c1eddbda79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff21894d08964a5abd5ef266ae23e723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1691f052937b4473a8e79f64554cdb19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e56dc9131998422bab4415c3eb74cb38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}