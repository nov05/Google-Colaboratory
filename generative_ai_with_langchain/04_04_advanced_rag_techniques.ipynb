{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nov05/Google-Colaboratory/blob/master/generative_ai_with_langchain/04_04_advanced_rag_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Notebook modified by nov05 on 2025-06-06   "
      ],
      "metadata": {
        "id": "ejlrg5WObhbh"
      },
      "id": "ejlrg5WObhbh"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain_core langchain_openai langchain_community\n",
        "## Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.24 langchain_core-0.3.64 langchain_openai-0.3.21\n",
        "## langsmith-0.3.45 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
        "## For FAISS\n",
        "!pip install jq  ## Successfully installed jq-1.8.0\n",
        "# !pip install faiss-gpu\n",
        "!pip install faiss-cpu  ## Successfully installed faiss-cpu-1.11.0"
      ],
      "metadata": {
        "id": "Iwi6V0zMd0K0"
      },
      "id": "Iwi6V0zMd0K0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u1vGg6g5sZN",
        "outputId": "a112bf20-b643-40f9-d9d8-27c76fe5a1fc"
      },
      "id": "-u1vGg6g5sZN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c04be25-aa04-44a3-9a84-5ed7513e9f30",
      "metadata": {
        "id": "6c04be25-aa04-44a3-9a84-5ed7513e9f30"
      },
      "source": [
        "**Make sure you load the API keys for cloud providers!**\n",
        "\n",
        "You can set your environment keys yourself or use a script. Please note that since keys are private, they are not included in the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d587a3c9-4cc8-4b1d-9b64-93f323a9b991",
      "metadata": {
        "id": "d587a3c9-4cc8-4b1d-9b64-93f323a9b991"
      },
      "outputs": [],
      "source": [
        "# # setting the environment variables, the keys\n",
        "# import sys\n",
        "# import os\n",
        "# sys.path.insert(0, os.path.abspath('..'))\n",
        "# from config import set_environment  ## local import\n",
        "# # for the keys - as explained early in chapter 2\n",
        "# set_environment()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "l7HwcCW04HQh"
      },
      "id": "l7HwcCW04HQh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a4a472b9-332a-44aa-93a8-e45cb3b0cedc",
      "metadata": {
        "id": "a4a472b9-332a-44aa-93a8-e45cb3b0cedc"
      },
      "source": [
        "# Query Expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16334895-b204-41e8-8403-e801b0fea004",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16334895-b204-41e8-8403-e801b0fea004",
        "outputId": "b0c68d2e-bb5d-4ab5-9576-d15120046adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given the user question: What are the effects of climate change?\n",
            "Generate three alternative versions that express the same information need but with different wording:\n",
            "1. How does climate change impact the environment?\n",
            "2. What consequences does climate change have on the planet?\n",
            "3. In what ways does climate change affect ecosystems and weather patterns?\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "expansion_template = \"\"\"Given the user question: {question}\n",
        "Generate three alternative versions that express the same information need but with different wording:\n",
        "1.\"\"\"\n",
        "expansion_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=expansion_template\n",
        ")\n",
        "llm = ChatOpenAI(temperature=0.7)\n",
        "expansion_chain = expansion_prompt | llm | StrOutputParser()\n",
        "# Generate expanded queries\n",
        "original_query = \"What are the effects of climate change?\"\n",
        "expanded_queries = expansion_chain.invoke(original_query)\n",
        "print(expansion_template.format(question=original_query), expanded_queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129a49f0-fcd6-4865-aa69-2f0b7977fce2",
      "metadata": {
        "id": "129a49f0-fcd6-4865-aa69-2f0b7977fce2"
      },
      "source": [
        "# Hypothetical Document Embeddings (HyDE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **ChatGPT**:   \n",
        "  Hypothetical Document Embeddings (HyDE) is a technique introduced to improve semantic search and retrieval tasks using large language models (LLMs). It works by generating a synthetic answer (a \"hypothetical document\") for a user query, and then embedding that answer—not the original query itself—to search a vector database.\n",
        "\n",
        "  When to Use HyDE:\n",
        "  * Your queries are short, vague, or not semantically rich.\n",
        "  * You want to increase recall in retrieval-augmented generation (RAG).\n",
        "  * You’re building a chatbot or QA system where LLM context is critical.  "
      ],
      "metadata": {
        "id": "g35TFPnG6Xqj"
      },
      "id": "g35TFPnG6Xqj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad253e09-2260-412c-bc4d-22f784d84f83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad253e09-2260-412c-bc4d-22f784d84f83",
        "outputId": "dbc92761-f7f7-4021-9261-b7d423beb9c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/20250602_langchai with generative ai/chapter4/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/20250602_langchai with generative ai/chapter4/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/20250602_langchai with generative ai/chapter4/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/20250602_langchai with generative ai/chapter4/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/20250602_langchai with generative ai/chapter4/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "\n",
        "prefix = \"/content/drive/MyDrive/Colab Notebooks/20250602_langchai with generative ai/chapter4/\"\n",
        "loader = JSONLoader(\n",
        "    file_path=prefix+\"knowledge_base.json\",\n",
        "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
        "    text_content=True\n",
        ")\n",
        "documents = loader.load()\n",
        "vector_db = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf9ebb7-899b-4a9c-b0b5-0e42ee277afe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bf9ebb7-899b-4a9c-b0b5-0e42ee277afe",
        "outputId": "c444102d-6c40-4366-e5fb-0fa626f271a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.\n",
            "Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\n",
            "Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Create prompt for generating hypothetical document\n",
        "hyde_template = \"\"\"Based on the question: {question}\n",
        "Write a passage that could contain the answer to this question:\"\"\"\n",
        "hyde_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=hyde_template\n",
        ")\n",
        "llm = ChatOpenAI(temperature=0.2)  ## low in temperature leans factual\n",
        "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Generate hypothetical document\n",
        "query = \"What dietary changes can reduce carbon footprint?\"\n",
        "hypothetical_doc = hyde_chain.invoke(query)\n",
        "\n",
        "# Use the hypothetical document for retrieval\n",
        "embeddings = OpenAIEmbeddings()\n",
        "embedded_query = embeddings.embed_query(hypothetical_doc)\n",
        "results = vector_db.similarity_search_by_vector(embedded_query, k=3)\n",
        "for result in results:\n",
        "    print(result.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(hyde_template.format(question=query))\n",
        "print(hypothetical_doc, \"\\n\")\n",
        "for i,result in enumerate(results):\n",
        "    print(f\"{i+1}. {result.page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykA1TuCBMrfD",
        "outputId": "6eda49ef-d790-4dd7-d28d-69d16b89dcfd"
      },
      "id": "ykA1TuCBMrfD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the question: What dietary changes can reduce carbon footprint?\n",
            "Write a passage that could contain the answer to this question:\n",
            "One way to reduce your carbon footprint through dietary changes is to incorporate more plant-based foods into your diet. Plant-based foods generally have a lower carbon footprint compared to animal-based foods, as they require less resources and produce fewer greenhouse gas emissions during production. By reducing your consumption of meat and dairy products and opting for more fruits, vegetables, grains, and legumes, you can significantly decrease your carbon footprint. Additionally, choosing locally sourced and seasonal foods can also help reduce the environmental impact of your diet, as it reduces the energy and emissions associated with transportation. Overall, making small changes to your diet can have a big impact on reducing your carbon footprint and contributing to a more sustainable food system. \n",
            "\n",
            "1. Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.\n",
            "2. Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\n",
            "3. Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732d7d39-cc5a-4a07-b67b-aac2206ae79e",
      "metadata": {
        "id": "732d7d39-cc5a-4a07-b67b-aac2206ae79e"
      },
      "source": [
        "# Contextual Compression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90349445-a56d-48e8-a02b-930e6d7c22c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90349445-a56d-48e8-a02b-930e6d7c22c7",
        "outputId": "20fe2284-2f11-4f8c-ede7-536310608ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\n",
            "BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018.\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)  ## ⚠️ lean factual\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "# Create a basic retriever from the vector store\n",
        "base_retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=base_retriever\n",
        ")\n",
        "compressed_docs = compression_retriever.invoke(\"How do transformers work?\")\n",
        "for compressed_doc in compressed_docs:\n",
        "    print(compressed_doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "672bc16d-96f0-415d-bb5e-3cefd0638d5f",
      "metadata": {
        "id": "672bc16d-96f0-415d-bb5e-3cefd0638d5f"
      },
      "source": [
        "# Maximum Marginal Relevance (MMR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1d5b65-bd5e-41f4-a523-4b9634f55b3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d1d5b65-bd5e-41f4-a523-4b9634f55b3b",
        "outputId": "a08da07a-2601-431d-900f-5ef13d403a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS.from_documents(documents, embeddings)\n",
        "mmr_results = vector_db.max_marginal_relevance_search(\n",
        "    query=\"What are transformer models?\",\n",
        "    k=1, # Number of documents to return\n",
        "    fetch_k=3, # Number of documents to initially fetch\n",
        "    lambda_mult=0.5 # Diversity parameter (0 = max diversity, 1 = max relevance)\n",
        ")\n",
        "print(mmr_results[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "216aa562-d716-4cc1-a713-eaf29a0912b5",
      "metadata": {
        "id": "216aa562-d716-4cc1-a713-eaf29a0912b5"
      },
      "source": [
        "# Source Attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac233fb-0d93-4874-b372-098e7859afe0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ac233fb-0d93-4874-b372-098e7859afe0",
        "outputId": "e8557873-c23d-429c-e2fa-ea4460ba0816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer models are a type of neural network architecture that relies on self-attention mechanisms to process input data. They were first introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017 [1]. One example of a transformer model is BERT, which utilizes bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks [2]. Another example is the GPT series of models, which are autoregressive transformers that predict the next token based on previous tokens [3].\n",
            "\n",
            "Reference list:\n",
            "[1] Neural Network Review 2021, page 42\n",
            "[2] Introduction to NLP, page 137\n",
            "[3] Large Language Models Survey, page 89\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Example documents\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.\",\n",
        "        metadata={\"source\": \"Neural Network Review 2021\", \"page\": 42}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"BERT uses bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks.\",\n",
        "        metadata={\"source\": \"Introduction to NLP\", \"page\": 137}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"GPT models are autoregressive transformers that predict the next token based on previous tokens.\",\n",
        "        metadata={\"source\": \"Large Language Models Survey\", \"page\": 89}\n",
        "    )\n",
        "]\n",
        "# Create a vector store and retriever\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vector_store = FAISS.from_documents(documents, embeddings)\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Source attribution prompt template\n",
        "attribution_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a precise AI assistant that provides well-sourced information.\n",
        "Answer the following question based ONLY on the provided sources. For each fact or claim in your answer,\n",
        "include a citation using [1], [2], etc. that refers to the source. Include a numbered reference list at the end.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Sources:\n",
        "{sources}\n",
        "\n",
        "Your answer:\n",
        "\"\"\")\n",
        "\n",
        "# Create a source-formatted string from documents\n",
        "def format_sources_with_citations(docs):\n",
        "    formatted_sources = []\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        source_info = f\"[{i}] {doc.metadata.get('source', 'Unknown source')}\"\n",
        "        if doc.metadata.get('page'):\n",
        "            source_info += f\", page {doc.metadata['page']}\"\n",
        "        formatted_sources.append(f\"{source_info}\\n{doc.page_content}\")\n",
        "    return \"\\n\\n\".join(formatted_sources)\n",
        "\n",
        "# Build the RAG chain with source attribution\n",
        "def generate_attributed_response(question):\n",
        "    # Retrieve relevant documents\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    # Format sources with citation numbers\n",
        "    sources_formatted = format_sources_with_citations(retrieved_docs)\n",
        "    # Create the attribution chain using LCEL\n",
        "    attribution_chain = (\n",
        "        attribution_prompt\n",
        "        | ChatOpenAI(temperature=0)\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    # Generate the response with citations\n",
        "    response = attribution_chain.invoke({\n",
        "        \"question\": question,\n",
        "        \"sources\": sources_formatted\n",
        "    })\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "question = \"How do transformer models work and what are some examples?\"\n",
        "attributed_answer = generate_attributed_response(question)\n",
        "print(attributed_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ce595af-3847-4903-bbc5-8c1b19ada5db",
      "metadata": {
        "id": "1ce595af-3847-4903-bbc5-8c1b19ada5db"
      },
      "source": [
        "# Self-consistency Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45eede9-bdbb-4625-bfc8-f340fef38ac3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e45eede9-bdbb-4625-bfc8-f340fef38ac3",
        "outputId": "43843f13-481d-4d64-ed12-ffcf677746a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"claims\": [\n",
            "        {\n",
            "            \"claim\": \"The transformer architecture was introduced by OpenAI in 2018\",\n",
            "            \"status\": \"contradicted\",\n",
            "            \"evidence\": \"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017.\",\n",
            "            \"explanation\": \"The claim is contradicted by the fact that the transformer architecture was actually introduced in 2017 by Vaswani et al., not by OpenAI in 2018.\"\n",
            "        },\n",
            "        {\n",
            "            \"claim\": \"The transformer architecture uses recurrent neural networks\",\n",
            "            \"status\": \"contradicted\",\n",
            "            \"evidence\": \"It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\",\n",
            "            \"explanation\": \"The claim is contradicted by the fact that the transformer architecture does not use recurrent neural networks, but rather self-attention mechanisms.\"\n",
            "        },\n",
            "        {\n",
            "            \"claim\": \"BERT is a transformer model developed by Google\",\n",
            "            \"status\": \"fully_supported\",\n",
            "            \"evidence\": \"BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.\",\n",
            "            \"explanation\": \"This claim is fully supported by the provided context.\"\n",
            "        }\n",
            "    ],\n",
            "    \"fully_grounded\": false,\n",
            "    \"issues_identified\": [\"The answer contains incorrect information about the introduction of the transformer architecture and its use of recurrent neural networks.\"]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing import List, Dict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def verify_response_accuracy(\n",
        "    retrieved_docs: List[Document],\n",
        "    generated_answer: str,\n",
        "    llm: ChatOpenAI = None\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Verify if a generated answer is fully supported by the retrieved documents.\n",
        "    Args:\n",
        "        retrieved_docs: List of documents used to generate the answer\n",
        "        generated_answer: The answer produced by the RAG system\n",
        "        llm: Language model to use for verification\n",
        "    Returns:\n",
        "        Dictionary containing verification results and any identified issues\n",
        "    \"\"\"\n",
        "    if llm is None:\n",
        "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "    # Create context from retrieved documents\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Define verification prompt - fixed to avoid JSON formatting issues in the template\n",
        "    verification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "    As a fact-checking assistant, verify whether the following answer is fully supported\n",
        "    by the provided context. Identify any statements that are not supported or contradict the context.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Answer to verify:\n",
        "    {answer}\n",
        "\n",
        "    Perform a detailed analysis with the following structure:\n",
        "    1. List any factual claims in the answer\n",
        "    2. For each claim, indicate whether it is:\n",
        "       - Fully supported (provide the supporting text from context)\n",
        "       - Partially supported (explain what parts lack support)\n",
        "       - Contradicted (identify the contradiction)\n",
        "       - Not mentioned in context\n",
        "    3. Overall assessment: Is the answer fully grounded in the context?\n",
        "\n",
        "    Return your analysis in JSON format with the following structure:\n",
        "    {{\n",
        "      \"claims\": [\n",
        "        {{\n",
        "          \"claim\": \"The factual claim\",\n",
        "          \"status\": \"fully_supported|partially_supported|contradicted|not_mentioned\",\n",
        "          \"evidence\": \"Supporting or contradicting text from context\",\n",
        "          \"explanation\": \"Your explanation\"\n",
        "        }}\n",
        "      ],\n",
        "      \"fully_grounded\": true|false,\n",
        "      \"issues_identified\": [\"List any specific issues\"]\n",
        "    }}\n",
        "    \"\"\")\n",
        "\n",
        "    # Create verification chain using LCEL\n",
        "    verification_chain = (\n",
        "        verification_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Run verification\n",
        "    result = verification_chain.invoke({\n",
        "        \"context\": context,\n",
        "        \"answer\": generated_answer\n",
        "    })\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "retrieved_docs = [\n",
        "    Document(\n",
        "        page_content=\"The transformer architecture was introduced in the paper \"\n",
        "        \"'Attention Is All You Need' by Vaswani et al. in 2017. It relies on \"\n",
        "        \"self-attention mechanisms instead of recurrent or convolutional neural networks.\"\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"BERT is a transformer-based model developed by Google that \"\n",
        "        \"uses masked language modeling and next sentence prediction as pre-training objectives.\"\n",
        "    )\n",
        "]\n",
        "generated_answer = (\n",
        "    \"The transformer architecture was introduced by OpenAI in 2018 and uses \"\n",
        "    \"recurrent neural networks. BERT is a transformer model developed by Google.\"\n",
        ")\n",
        "verification_result = verify_response_accuracy(retrieved_docs, generated_answer)\n",
        "print(verification_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2074f2c8-bdf6-4952-94a2-010cfdb8e4eb",
      "metadata": {
        "id": "2074f2c8-bdf6-4952-94a2-010cfdb8e4eb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}