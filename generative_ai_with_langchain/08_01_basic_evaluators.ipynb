{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nov05/Google-Colaboratory/blob/master/generative_ai_with_langchain/08_01_basic_evaluators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Notebook modified by nov05 on 2025-06-13  "
      ],
      "metadata": {
        "id": "O169J_cUtYLL"
      },
      "id": "O169J_cUtYLL"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain langchain-mistralai langchain-community langchain_openai"
      ],
      "metadata": {
        "id": "_MnQIbnGyAzv"
      },
      "id": "_MnQIbnGyAzv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.25 langchain-core-0.3.65 langchain-mistralai-0.2.10 langsmith-0.3.45 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0 langchain_openai-0.3.23*  "
      ],
      "metadata": {
        "id": "8RyhnHynyebk"
      },
      "id": "8RyhnHynyebk"
    },
    {
      "cell_type": "markdown",
      "id": "bcd1a0dd-25b3-4d61-939c-cef759afced6",
      "metadata": {
        "id": "bcd1a0dd-25b3-4d61-939c-cef759afced6"
      },
      "source": [
        "**Make sure you load the API keys for cloud providers!**\n",
        "\n",
        "You can set your environment keys yourself or use a script. Please note that since keys are private, they are not included in the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e68f8b4-67ce-4fd6-9369-0884b592de41",
      "metadata": {
        "id": "1e68f8b4-67ce-4fd6-9369-0884b592de41"
      },
      "outputs": [],
      "source": [
        "# # setting the environment variables, the keys\n",
        "# import sys\n",
        "# import os\n",
        "# sys.path.insert(0, os.path.abspath('..'))\n",
        "# from config import set_environment\n",
        "# # for the keys - as explained early in chapter 2\n",
        "# set_environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ¢ **Basica Evalutors**"
      ],
      "metadata": {
        "id": "-ZtYfiDNtjKN"
      },
      "id": "-ZtYfiDNtjKN"
    },
    {
      "cell_type": "markdown",
      "id": "7644c0f3-1603-4748-89d2-b44106600acd",
      "metadata": {
        "id": "7644c0f3-1603-4748-89d2-b44106600acd"
      },
      "source": [
        "## ðŸ‘‰ **Exact Match Evaluation**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "04d7be71-7ba7-4e81-9d04-b23f01972441",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04d7be71-7ba7-4e81-9d04-b23f01972441",
        "outputId": "0fa7a9a5-b5ed-43e6-c267-e2420d85acc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match result (correct answer): {'score': 1}\n",
            "Exact match result (incorrect answer): {'score': 0}\n"
          ]
        }
      ],
      "source": [
        "from langchain.evaluation import load_evaluator, ExactMatchStringEvaluator\n",
        "\n",
        "prompt = \"What is the current Federal Reserve interest rate?\"\n",
        "reference_answer = \"0.25%\"  # Suppose this is the correct answer.\n",
        "\n",
        "# Example predictions:\n",
        "prediction_correct = \"0.25%\"\n",
        "prediction_incorrect = \"0.50%\"\n",
        "\n",
        "# Initialize an Exact Match evaluator that ignores case differences.\n",
        "exact_evaluator = ExactMatchStringEvaluator(ignore_case=True)\n",
        "\n",
        "# Evaluate the correct prediction.\n",
        "exact_result_correct = exact_evaluator.evaluate_strings(\n",
        "    prediction=prediction_correct, reference=reference_answer\n",
        ")\n",
        "print(\"Exact match result (correct answer):\", exact_result_correct)\n",
        "\n",
        "# Evaluate an incorrect prediction.\n",
        "exact_result_incorrect = exact_evaluator.evaluate_strings(\n",
        "    prediction=prediction_incorrect, reference=reference_answer\n",
        ")\n",
        "print(\"Exact match result (incorrect answer):\", exact_result_incorrect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98de484a-fea6-4bb0-90ac-785aaa35588b",
      "metadata": {
        "id": "98de484a-fea6-4bb0-90ac-785aaa35588b"
      },
      "source": [
        "## ðŸ‘‰ **LLM-as-Judge Evaluation**  \n",
        "\n",
        "* Get Mistral AI API keys at https://admin.mistral.ai/organization/api-keys  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9ebb0b48-46b9-409d-b1e5-6352e8ef3485",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ebb0b48-46b9-409d-b1e5-6352e8ef3485",
        "outputId": "7c639e8d-9fe0-4ed3-c6df-4b7ec7aecec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.evaluation.scoring.eval_chain:This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finance Evaluation Result:\n",
            "{'reasoning': 'The response provided by the AI assistant is clear and concise, '\n",
            "              \"but it lacks depth and context. The Federal Reserve's interest \"\n",
            "              'rate can change periodically, and without a specific date '\n",
            "              'reference, the information might not be accurate or up-to-date. '\n",
            "              'Additionally, the response does not provide any insight into '\n",
            "              'the implications of the interest rate or how it might affect '\n",
            "              'the economy or consumers.\\n'\n",
            "              '\\n'\n",
            "              'Rating: [[4]]',\n",
            " 'score': 4}\n",
            "CPU times: user 19.7 ms, sys: 3.34 ms, total: 23 ms\n",
            "Wall time: 2.05 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain.evaluation.scoring import ScoreStringEvalChain\n",
        "from google.colab import userdata\n",
        "from pprint import pprint\n",
        "\n",
        "# Initialize the evaluator LLM\n",
        "llm = ChatMistralAI(\n",
        "    model=\"mistral-large-latest\",\n",
        "    api_key=userdata.get('MISTRAL_API_KEY'),  ## âœ… mandatory\n",
        "    temperature=0,\n",
        ")\n",
        "# Create the ScoreStringEvalChain from the LLM\n",
        "chain = ScoreStringEvalChain.from_llm(llm=llm)\n",
        "\n",
        "# Define the finance-related input, prediction, and reference answer\n",
        "finance_input = \"What is the current Federal Reserve interest rate?\"\n",
        "finance_prediction = \"The current interest rate is 0.25%.\"\n",
        "finance_reference = \"The Federal Reserve's current interest rate is 0.25%.\"\n",
        "\n",
        "# Evaluate the prediction using the scoring chain\n",
        "result_finance = chain.evaluate_strings(\n",
        "    input=finance_input,\n",
        "    prediction=finance_prediction,\n",
        ")\n",
        "print(\"Finance Evaluation Result:\")\n",
        "pprint(result_finance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2abb4d87-aa99-400c-8357-9f528af06e04",
      "metadata": {
        "id": "2abb4d87-aa99-400c-8357-9f528af06e04"
      },
      "source": [
        "## ðŸ‘‰ **LLM-as-Judge Evaluation with Reference Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the `~langchain-openai` package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`."
      ],
      "metadata": {
        "id": "9BMd9dMP8Ec_"
      },
      "id": "9BMd9dMP8Ec_"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cae0de7c-9426-439f-91a1-d4cead4e64ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cae0de7c-9426-439f-91a1-d4cead4e64ae",
        "outputId": "cf64f88b-da02-4b8d-adf1-0bca5a97e1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finance Evaluation Result (with reference):\n",
            "{'reasoning': \"The assistant's response is helpful, relevant, and correct. It \"\n",
            "              \"directly answers the user's question about the current Federal \"\n",
            "              'Reserve interest rate. The assistant provides the exact rate, '\n",
            "              'which is factual and accurate. However, the response lacks '\n",
            "              'depth as it does not provide any additional information or '\n",
            "              'context about the interest rate or the Federal Reserve. Rating: '\n",
            "              '[[8]]',\n",
            " 'score': 8}\n",
            "CPU times: user 535 ms, sys: 16.3 ms, total: 552 ms\n",
            "Wall time: 3.44 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.evaluation.scoring import LabeledScoreStringEvalChain\n",
        "\n",
        "# Initialize the evaluator LLM\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4\",\n",
        "    api_key=userdata.get('OPENAI_API_KEY'),  ## âœ… mandatory\n",
        "    temperature=0,\n",
        ")\n",
        "# Create the evaluation chain that can use reference answers\n",
        "labeled_chain = LabeledScoreStringEvalChain.from_llm(llm=llm)\n",
        "\n",
        "# Define the finance-related input, prediction, and reference answer\n",
        "finance_input = \"What is the current Federal Reserve interest rate?\"\n",
        "finance_prediction = \"The current interest rate is 0.25%.\"\n",
        "finance_reference = \"The Federal Reserve's current interest rate is 0.25%.\"\n",
        "\n",
        "# Evaluate the prediction against the reference\n",
        "labeled_result = labeled_chain.evaluate_strings(\n",
        "    input=finance_input,\n",
        "    prediction=finance_prediction,\n",
        "    reference=finance_reference,\n",
        ")\n",
        "print(\"Finance Evaluation Result (with reference):\")\n",
        "pprint(labeled_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6658969d-1e31-45c3-99a2-4a7127a1be0c",
      "metadata": {
        "id": "6658969d-1e31-45c3-99a2-4a7127a1be0c"
      },
      "source": [
        "## ðŸ‘‰ **Criteria Evaluation (Tone and Conciseness)**  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "29d00b3a-ace4-481e-85a2-391d4d6fe3ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29d00b3a-ace4-481e-85a2-391d4d6fe3ee",
        "outputId": "8f7941b4-9ed1-4dce-f2cf-a770278b5de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conciseness evaluation result:\n",
            "{'reasoning': 'The criterion is conciseness. This means the submission should '\n",
            "              'be brief and to the point.\\n'\n",
            "              '\\n'\n",
            "              'Looking at the submission, it provides a direct answer to the '\n",
            "              'question, stating that a normal blood pressure reading is '\n",
            "              'around 120/80 mmHg. This is a concise response to the '\n",
            "              'question.\\n'\n",
            "              '\\n'\n",
            "              'The submission also includes an additional sentence advising to '\n",
            "              \"follow a doctor's advice for personal health management. While \"\n",
            "              'this sentence is not directly related to the question, it is '\n",
            "              'still relevant and does not make the response overly lengthy or '\n",
            "              'verbose.\\n'\n",
            "              '\\n'\n",
            "              'Therefore, the submission can be considered concise.\\n'\n",
            "              '\\n'\n",
            "              'Y',\n",
            " 'score': 1,\n",
            " 'value': 'Y'}\n",
            "Friendliness evaluation result:\n",
            "{'reasoning': 'The criterion is to assess whether the response is written in a '\n",
            "              'friendly and approachable tone. The submission provides the '\n",
            "              'information in a straightforward manner and ends with a '\n",
            "              \"suggestion to follow doctor's advice for personal health \"\n",
            "              'management. This suggestion can be seen as a friendly advice, '\n",
            "              \"showing concern for the reader's health. Therefore, the \"\n",
            "              'submission can be considered as written in a friendly and '\n",
            "              'approachable tone.\\n'\n",
            "              '\\n'\n",
            "              'Y',\n",
            " 'score': 1,\n",
            " 'value': 'Y'}\n",
            "CPU times: user 38.9 ms, sys: 6.52 ms, total: 45.5 ms\n",
            "Wall time: 7.73 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from langchain.evaluation import load_evaluator\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "evaluation_llm = ChatOpenAI(\n",
        "    model=\"gpt-4\",\n",
        "    api_key=userdata.get('OPENAI_API_KEY'),  ## âœ… mandatory\n",
        "    temperature=0,\n",
        ")\n",
        "prompt_health = \"What is a healthy blood pressure range for adults?\"\n",
        "prediction_health = (\n",
        "    \"A normal blood pressure reading is typically around 120/80 mmHg. \"\n",
        "    \"It's important to follow your doctor's advice for personal health management!\"\n",
        ")\n",
        "# Evaluate conciseness\n",
        "conciseness_evaluator = load_evaluator(\n",
        "    \"criteria\",\n",
        "    criteria=\"conciseness\",\n",
        "    llm=evaluation_llm\n",
        ")\n",
        "conciseness_result = conciseness_evaluator.evaluate_strings(\n",
        "    prediction=prediction_health,\n",
        "    input=prompt_health\n",
        ")\n",
        "print(\"Conciseness evaluation result:\")\n",
        "pprint(conciseness_result)\n",
        "\n",
        "# Evaluate friendliness with custom criterion\n",
        "custom_friendliness = {\n",
        "    \"friendliness\": \"Is the response written in a friendly and approachable tone?\"\n",
        "}\n",
        "friendliness_evaluator = load_evaluator(\n",
        "    \"criteria\",\n",
        "    criteria=custom_friendliness,\n",
        "    llm=evaluation_llm\n",
        ")\n",
        "friendliness_result = friendliness_evaluator.evaluate_strings(\n",
        "    prediction=prediction_health,\n",
        "    input=prompt_health\n",
        ")\n",
        "print(\"Friendliness evaluation result:\")\n",
        "pprint(friendliness_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e40b9d-f51d-4c0a-8b63-f87efe960c91",
      "metadata": {
        "id": "75e40b9d-f51d-4c0a-8b63-f87efe960c91"
      },
      "source": [
        "## ðŸ‘‰ **JSON Format Validation**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6ab0e38c-b038-4b90-8624-6ffc05f1e014",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ab0e38c-b038-4b90-8624-6ffc05f1e014",
        "outputId": "ad68bf46-dc22-4a61-bd97-c2e02a2f0fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON validity result (valid): {'score': 1}\n",
            "JSON validity result (invalid):\n",
            "{'reasoning': 'Expecting property name enclosed in double quotes: line 1 '\n",
            "              'column 63 (char 62)',\n",
            " 'score': 0}\n",
            "CPU times: user 652 Âµs, sys: 0 ns, total: 652 Âµs\n",
            "Wall time: 638 Âµs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from langchain.evaluation import JsonValidityEvaluator\n",
        "\n",
        "# Initialize the JSON validity evaluator.\n",
        "json_validator = JsonValidityEvaluator()\n",
        "\n",
        "valid_json_output = '{\"company\": \"Acme Corp\", \"revenue\": 1000000, \"profit\": 200000}'\n",
        "invalid_json_output = '{\"company\": \"Acme Corp\", \"revenue\": 1000000, \"profit\": 200000,}'\n",
        "\n",
        "# Evaluate the valid JSON.\n",
        "valid_result = json_validator.evaluate_strings(\n",
        "    prediction=valid_json_output\n",
        ")\n",
        "print(\"JSON validity result (valid):\", valid_result)\n",
        "\n",
        "# Evaluate the invalid JSON.\n",
        "invalid_result = json_validator.evaluate_strings(\n",
        "    prediction=invalid_json_output\n",
        ")\n",
        "print(\"JSON validity result (invalid):\")\n",
        "pprint(invalid_result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}