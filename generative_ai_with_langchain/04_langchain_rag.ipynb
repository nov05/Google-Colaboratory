{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nov05/Google-Colaboratory/blob/master/generative_ai_with_langchain/04_langchain_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Notebook modified by nov05 on 2025-06-13  "
      ],
      "metadata": {
        "id": "kofMi2OTMcE3"
      },
      "id": "kofMi2OTMcE3"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain langchain_community langchain_openai langchain_google_genai \\\n",
        "    langchain_chroma langchain_core langchain_text_splitters\n",
        "## Executing time 56s\n",
        "## ‚ö†Ô∏è Restart session after installation"
      ],
      "metadata": {
        "id": "BNiVsnrSMfu9"
      },
      "id": "BNiVsnrSMfu9",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.12 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 fastapi-0.115.9 filetype-1.2.0 google-ai-generativelanguage-0.6.18 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 kubernetes-33.1.0 langchain_chroma-0.2.4 langchain_community-0.3.25 langchain_core-0.3.65 langchain_google_genai-2.1.5 langchain_openai-0.3.22 langsmith-0.3.45 marshmallow-3.26.1 mmh3-5.1.0 mypy-extensions-1.1.0 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-instrumentation-0.55b1 opentelemetry-instrumentation-asgi-0.55b1 opentelemetry-instrumentation-fastapi-0.55b1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 opentelemetry-util-http-0.55b1 overrides-7.7.0 posthog-4.8.0 pydantic-settings-2.9.1 pypika-0.48.9 python-dotenv-1.1.0 starlette-0.45.3 typing-inspect-0.9.0 uvloop-0.21.0 watchfiles-1.0.5*  "
      ],
      "metadata": {
        "id": "8OGA8tcANU2d"
      },
      "id": "8OGA8tcANU2d"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-zUirBy5TZ0",
        "outputId": "9c251ec4-f950-4e35-cbd0-e03493609eca"
      },
      "id": "H-zUirBy5TZ0",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chapter_folder_path = \"/content/drive/MyDrive/Colab Notebooks/20250602_langchai with generative ai/chapter7\"\n",
        "pickle_path = \"/content/drive/MyDrive/Colab Notebooks/20250602_langchai with generative ai/chapter7/documents.pkl\"\n",
        "## Check the documents pickled file\n",
        "!ls -1 \"{chapter_folder_path}\" | grep \"pkl\"\n",
        "## Count the number of files and directories in the folder\n",
        "!ls -1 \"{chapter_folder_path}\" | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIRXQ_m4G8HR",
        "outputId": "a36ae3d2-9c20-421a-d4b9-124ad1f612f6"
      },
      "id": "OIRXQ_m4G8HR",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "documents.pkl\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cbf9ba9-81c6-4218-a11c-41ff15651d75",
      "metadata": {
        "id": "6cbf9ba9-81c6-4218-a11c-41ff15651d75"
      },
      "outputs": [],
      "source": [
        "# # setting the environment variables\n",
        "# import sys\n",
        "# import os\n",
        "# sys.path.insert(0, os.path.abspath('..'))\n",
        "# from config import set_environment\n",
        "# set_environment()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"USER_AGENT\"] = \"nov05-langchain-docusaurus/0.1\""
      ],
      "metadata": {
        "id": "1spFHO1I2WZm"
      },
      "id": "1spFHO1I2WZm",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ **Build a RAG on a documentation website**  "
      ],
      "metadata": {
        "id": "AejDqbriLOZ0"
      },
      "id": "AejDqbriLOZ0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `Docusaurus` is a popular open-source static site generator optimized for building documentation websites (used by `Meta`, `React`, `Redux`, etc.). It typically stores docs as markdown files in a `docs/` directory.  \n",
        "\n",
        "* **ChatGPT:**  \n",
        "\n",
        "  The `USER_AGENT` environment variable is used by `langchain_community` (and underlying HTTP clients like `requests`) to **identify your application** when making web requests‚Äîjust like how a browser says \"I'm Chrome\" or \"I'm Firefox\".\n",
        "\n",
        "  * Recommended Best Practice\n",
        "\n",
        "    Set a meaningful value that identifies **you or your application**, in a polite and responsible way. This helps server admins understand who‚Äôs accessing their site and why‚Äîespecially important for ethical scraping.\n",
        "\n",
        "  * Example Values\n",
        "\n",
        "| Purpose                     | Example `USER_AGENT` Value                                                   |\n",
        "| --------------------------- | ---------------------------------------------------------------------------- |\n",
        "| Personal project / testing  | `\"MyName-langchain-docusaurus/0.1\"`                                          |\n",
        "| Organization usage          | `\"AcmeCorpLangchainBot/1.0 (contact: dev@acmecorp.com)\"`                     |\n",
        "| Research purpose            | `\"ResearchBot-LangChain/1.0 (university.edu; contact: name@university.edu)\"` |\n",
        "| Anonymous (not recommended) | `\"langchain/0.1\"`                                                            |\n",
        "\n",
        "* **ChatGPT:**  \n",
        "\n",
        "  `nest_asyncio.apply()` makes it safe to call `DocusaurusLoader.load()` (which may run async code under the hood) in environments where the event loop is already running, avoiding a RuntimeError.\n",
        "\n",
        "* Check OpenAI billing at https://platform.openai.com/settings/organization/billing/overview. Make sure you have enough credit balance to convert the documents to embeddings.      \n",
        "\n",
        "  * `text-embedding-3-small`: \\$0.02 per 1M tokens   \n",
        "  * `text-embedding-3-large`: \\$0.13 per 1M tokens    \n",
        "  \n",
        "  <img src=\"https://raw.githubusercontent.com/nov05/pictures/refs/heads/master/generative_ai_with_langchain/2025-06-13%2015_27_23-openai%20billing.jpg\" width=800>  "
      ],
      "metadata": {
        "id": "c7iKNuAjLtyV"
      },
      "id": "c7iKNuAjLtyV"
    },
    {
      "cell_type": "code",
      "source": [
        "## ‚ö†Ô∏è If you have pickled the documents (~30 MB), load from Google Drive file and skip the next cell\n",
        "## Or download the file at https://drive.google.com/file/d/1YsdZa7NsO-LmwNyTN0VKKDuXYS2yQi6U\n",
        "## !wget https://drive.google.com/uc?id=1YsdZa7NsO-LmwNyTN0VKKDuXYS2yQi6U -O document.pkl\n",
        "# import pickle\n",
        "# with open(pickle_path, \"rb\") as f:\n",
        "#     documents = pickle.load(f)"
      ],
      "metadata": {
        "id": "dPTB316s6Oto"
      },
      "id": "dPTB316s6Oto",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "28cd4fb8-126f-4c09-90dd-085c1f1b1c88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28cd4fb8-126f-4c09-90dd-085c1f1b1c88",
        "outputId": "06a5e1d1-5796-479f-8b3e-4ffca5447999"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching pages: 100%|##########| 1558/1558 [01:53<00:00, 13.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 28s, sys: 13.1 s, total: 4min 41s\n",
            "Wall time: 6min 19s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import nest_asyncio\n",
        "from langchain_community.document_loaders import DocusaurusLoader\n",
        "nest_asyncio.apply()\n",
        "loader = DocusaurusLoader(\"https://python.langchain.com\")\n",
        "documents = loader.load()\n",
        "## Fetching pages: 100%|##########| 1558/1558 [04:31<00:00,  5.75it/s]\n",
        "## CPU times: user 4min 38s, sys: 8.88 s, total: 4min 46s\n",
        "## Wall time: 9min 6s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Verify loading\n",
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDbEklhMLgwo",
        "outputId": "987f28a5-6f4c-4b6e-a172-a06e28ece891"
      },
      "id": "IDbEklhMLgwo",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://python.langchain.com/search/', 'loc': 'https://python.langchain.com/search/', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='\\n\\n\\n\\n\\nSearch the documentation | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchSearch the documentationCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Check document\n",
        "documents[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1NQok5t4CCM",
        "outputId": "6445189c-e755-481b-99b7-e49e58fb944a"
      },
      "id": "J1NQok5t4CCM",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'https://python.langchain.com/search/',\n",
              " 'loc': 'https://python.langchain.com/search/',\n",
              " 'changefreq': 'weekly',\n",
              " 'priority': '0.5'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Check document\n",
        "import re\n",
        "print(re.sub(r'\\n+', '\\n', documents[0].page_content.strip()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkTJwfnPR676",
        "outputId": "6a79c24c-d6ca-4e8f-ef2e-ee404076f15c"
      },
      "id": "FkTJwfnPR676",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search the documentation | ü¶úÔ∏èüîó LangChain\n",
            "Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchSearch the documentationCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# ## Pickle the documents\n",
        "# import sys, pickle\n",
        "# print(f\"Size of documents: {sys.getsizeof(documents)} bytes\")  ## 12728 bytes\n",
        "# with open(pickle_path, \"wb\") as f:\n",
        "#     pickle.dump(documents, f)  ## ~ 30 MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2asnERLk4yw2",
        "outputId": "e5c0776f-b4dd-4af4-b220-4101a32f1690"
      },
      "id": "2asnERLk4yw2",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12728\n",
            "CPU times: user 145 ms, sys: 54.7 ms, total: 199 ms\n",
            "Wall time: 380 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7d13d1a3-0e8a-446c-aea1-48a22d40c5ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d13d1a3-0e8a-446c-aea1-48a22d40c5ae",
        "outputId": "114d2b2a-0037-47c6-c7d1-a02031d01e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 135 ms, sys: 2.83 ms, total: 138 ms\n",
            "Wall time: 499 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.storage import LocalFileStore\n",
        "from google.colab import userdata\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "## Enable the \"OPENAI_API_KEY\" secret in Colab\n",
        "underlying_embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-large\",\n",
        "    api_key=userdata.get('OPENAI_API_KEY'),  ## ‚úÖ mandatory\n",
        ")\n",
        "## Avoiding unnecessary costs by caching the embeddings.\n",
        "embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
        "    underlying_embeddings,\n",
        "    store,\n",
        "    namespace=underlying_embeddings.model,\n",
        ")\n",
        "## CPU times: user 165 ms, sys: 2 ms, total: 167 ms\n",
        "## Wall time: 520 ms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6defc37d-b064-4059-a910-2c1fefa41ecb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6defc37d-b064-4059-a910-2c1fefa41ecb",
        "outputId": "9f1dd673-8a69-49df-9d36-89f92f718c2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 91389\n",
            "CPU times: user 5.01 s, sys: 130 ms, total: 5.14 s\n",
            "Wall time: 5.14 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400, # 1000\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "## Number of chunks: 91389\n",
        "## CPU times: user 5.26 s, sys: 121 ms, total: 5.38 s\n",
        "## Wall time: 5.49 s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e014eb47-e99d-4fb2-9c81-474bf7a9f846",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e014eb47-e99d-4fb2-9c81-474bf7a9f846",
        "outputId": "77521530-2d52-418f-d1f4-d327c52257f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14min 25s, sys: 34.5 s, total: 15min\n",
            "Wall time: 14min 31s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## This is when the embedding cache folder gets populated.\n",
        "from langchain_chroma import Chroma\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings\n",
        ")\n",
        "## You might encounter the following error.\n",
        "##     BadRequestError: Error code: 400 - {'error': {'message': 'Requested 427435 tokens, max 300000 tokens per request',\n",
        "##     'type': 'max_tokens_per_request', 'param': None, 'code': 'max_tokens_per_request'}}\n",
        "## CPU times: user 15min 13s, sys: 32.2 s, total: 15min 46s\n",
        "## Wall time: 16min 21s ‚ö†Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Count the number of files in the embedding cache folder: 37972\n",
        "!ls -1 \"/content/cache\" | wc -l\n",
        "## Get the folder size in MB: 2526\n",
        "!du -sm \"/content/cache\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScsEpnAQKtyI",
        "outputId": "960b3fe7-4a65-4973-a41f-6785a3a67f2a"
      },
      "id": "ScsEpnAQKtyI",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37972\n",
            "2526\t/content/cache\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b0d3865a-500c-4c7d-b7a2-dcfcfaae6edc",
      "metadata": {
        "id": "b0d3865a-500c-4c7d-b7a2-dcfcfaae6edc"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "llm = GoogleGenerativeAI(\n",
        "    # model_name=\"gemini-pro\"\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    api_key=userdata.get('GOOGLE_API_KEY'),  ## ‚úÖ mandatory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5b45d2b7-b566-45c6-b356-33385155ec89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b45d2b7-b566-45c6-b356-33385155ec89",
        "outputId": "02d1ad0e-08cc-4ffb-f4a8-c906288a3853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 111 ms, sys: 10.5 ms, total: 122 ms\n",
            "Wall time: 1.98 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# from langchain import hub\n",
        "from langsmith import Client\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "retriever = vector_store.as_retriever()\n",
        "## Check the prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
        "## Either method works.\n",
        "# prompt = hub.pull(\n",
        "#     \"rlm/rag-prompt\",\n",
        "#     api_key=userdata.get(\"LANGSMITH_API_KEY\")  ## optional, to suppress warnings\n",
        "# )\n",
        "client = Client(api_key=userdata.get(\"LANGSMITH_API_KEY\"))\n",
        "prompt = client.pull_prompt(\n",
        "    \"rlm/rag-prompt\",\n",
        "    include_model=True\n",
        ")\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "result = rag_chain.invoke(\"What is Task Decomposition?\")\n",
        "## CPU times: user 196 ms, sys: 25.8 ms, total: 222 ms\n",
        "## Wall time: 1.66 s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "print(textwrap.fill(result, width=72))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyrOnSTRC4ly",
        "outputId": "4c9e845b-cb23-43be-da71-36ba127dfe83"
      },
      "id": "oyrOnSTRC4ly",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task Decomposition is a technique used to break down complex tasks into\n",
            "smaller, more manageable steps. It often involves prompting models to\n",
            "\"think step by step,\" allowing for clearer reasoning and better\n",
            "performance on intricate problems. This can be achieved through methods\n",
            "like Chain of Thought prompting.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}